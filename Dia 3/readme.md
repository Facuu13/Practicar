# üìù D√≠a 3 ‚Äî Persistencia de telemetr√≠a en SQL (MySQL) y NoSQL (MongoDB)

¬°Vamos con el D√≠a 3! üìä  
**Tema**: Persistencia de telemetr√≠a en **SQL (MySQL)** y **NoSQL (MongoDB)**. Sin c√≥digo por ahora: solo **enunciado claro + tareas** y criterios de entrega.

---

## üìù Enunciado general

Tomando las lecturas del D√≠a 2 (o simuladas), vas a:

1. **Modelar** y **crear** el esquema en MySQL y en MongoDB.
2. **Insertar** lecturas desde un script (puede ser Python) en ambos motores.
3. **Consultar y agregar**: obtener promedios por hora y √∫ltimos N registros.
4. **Optimizar** con √≠ndices m√≠nimos necesarios.
5. **Exportar** resultados (CSV en SQL; JSON en NoSQL).

---

## Parte A ‚Äî MySQL (SQL)

### A1. Modelado y DDL

- Crear base de datos `greenhouse`.
- Crear tabla `readings` con las columnas:
  - `id` BIGINT AUTO_INCREMENT PRIMARY KEY
  - `device_id` VARCHAR(32) NOT NULL
  - `ts_utc` DATETIME(6) NOT NULL (UTC)
  - `temp_c` DECIMAL(5,2) NOT NULL
  - `hum_pct` DECIMAL(5,2) NOT NULL
- √çndices:
  - INDEX `idx_dev_ts` (`device_id`, `ts_utc`)
- **Reglas**:
  - `ts_utc` debe ser UTC (no hora local).
  - `temp_c` ‚àà [-40, 85]; `hum_pct` ‚àà [0, 100] (validar antes de insertar, no hace falta CHECK si no quer√©s).

**Entregable A1**: script SQL `schema_mysql.sql` con `CREATE DATABASE`, `CREATE TABLE`, e √≠ndices.

---

### A2. Inserciones (ingesta)

- Escribir un script que inserte **al menos 100 lecturas** para `dev001` y **50 para dev002** (total ‚â•150 filas).
- Los timestamps deben cubrir **‚â•3 horas** para que tenga sentido agrupar por hora.

**Entregable A2**: `ingest_mysql.py` (o el lenguaje que elijas) que:
- Conecta a MySQL.
- Inserta en `readings(device_id, ts_utc, temp_c, hum_pct)`.
- Imprime cu√°ntas filas insert√≥.

---

### A3. Consultas

1. **√öltimas 10 lecturas** de `dev001`, ordenadas descendente por `ts_utc`.
2. **Promedio por hora** (temp y hum) de `dev001` en un rango `[T0, T1]`:
   - Resultado: `hour_ts, avg_temp, avg_hum` (hora ‚Äúredondeada‚Äù/truncada a la hora).
3. **Top 3 horas m√°s calientes** (mayor `avg_temp`) para `dev001` en todo el per√≠odo insertado.

**Entregable A3**: `queries_mysql.sql` con 3 `SELECT` que resuelvan lo anterior.

---

### A4. Exportar CSV

- Exportar el resultado del punto A3(2) a `hourly_averages_mysql.csv`.

**Entregable A4**: el archivo `hourly_averages_mysql.csv` generado (o el script que lo genere).

---

### A5. Performance m√≠nima

- Explicar en un README corto por qu√© el √≠ndice (`device_id`, `ts_utc`) es suficiente para tus consultas.
- (Opcional) medir tiempos con y sin √≠ndice (peque√±a nota).

**Entregable A5**: `README_mysql.md` (m√°x 10 l√≠neas).

---

## Parte B ‚Äî MongoDB (NoSQL)

### B1. Modelado (colecci√≥n + √≠ndice)

- Base/DB: `greenhouse`.
- Colecci√≥n: `readings`.
- Documento por lectura (ejemplo):
  ```json
  {
    "device_id": "dev001",
    "ts_utc": { "$date": "2025-09-16T15:10:00Z" },
    "temp_c": 23.45,
    "hum_pct": 51.20
  }

  ## Parte B ‚Äî MongoDB (NoSQL)

### B1. Modelado (colecci√≥n + √≠ndice)

* Base/DB: `greenhouse`.
* Colecci√≥n: `readings`.
* Documento por lectura (ejemplo):

  ```json
  {
    "device_id": "dev001",
    "ts_utc": { "$date": "2025-09-16T15:10:00Z" },
    "temp_c": 23.45,
    "hum_pct": 51.20
  }
  ```
* √çndices:

  * Compuesto `{ device_id: 1, ts_utc: 1 }`.

**Entregable B1**: `schema_mongo.txt` describiendo la colecci√≥n e √≠ndices (y/o script `create_index.js`).

---

### B2. Inserciones (ingesta)

* Insertar **‚â•150 documentos** (al menos 100 de `dev001` y 50 de `dev002`), cubriendo ‚â•3 horas.
* Validar rangos antes de insertar (mismo criterio que SQL).

**Entregable B2**: `ingest_mongo.py` (o `mongoimport` con un JSON preparado).

---

### B3. Consultas (Aggregation)

1. **√öltimas 10 lecturas** de `dev001` (orden desc por `ts_utc`).
2. **Promedio por hora** para `dev001` en rango `[T0, T1]`:

   * Pipeline con `$match` (por device y rango), `$group` por hora (usar `$dateTrunc` o `$dateToString`), y `$project`.
   * Resultado: `hour_ts, avg_temp, avg_hum`.
3. **Top 3 horas m√°s calientes** (mayor promedio de `temp_c`) para `dev001`.

**Entregable B3**: `aggregations_mongo.js` con los pipelines.

---

### B4. Exportar JSON

* Export√° el resultado del punto B3(2) a `hourly_averages_mongo.json`.

**Entregable B4**: archivo exportado (o script que lo genere).

---

### B5. Performance m√≠nima

* Justificar brevemente el √≠ndice `{ device_id: 1, ts_utc: 1 }` para `$match` + sort/paginaci√≥n.
* (Opcional) agregar un ejemplo de paginaci√≥n por `_id` o `ts_utc`.

**Entregable B5**: `README_mongo.md` (m√°x 10 l√≠neas).

---

## Criterios de aceptaci√≥n (checklist r√°pido)

* [ ] MySQL: tabla creada con √≠ndice compuesto.
* [ ] MySQL: ‚â•150 filas insertadas repartidas en ‚â•3h y ‚â•2 device\_id.
* [ ] MySQL: 3 consultas pedidas + CSV de promedios por hora.
* [ ] MongoDB: colecci√≥n con √≠ndice compuesto.
* [ ] MongoDB: ‚â•150 docs, ‚â•3h, ‚â•2 device\_id.
* [ ] MongoDB: 3 pipelines pedidas + JSON de promedios por hora.
* [ ] README (SQL y Mongo) explicando por qu√© el √≠ndice sirve.

---

## Extras (opcionales, si quer√©s subir dificultad)

* **Ventanas m√≥viles**: promedio m√≥vil de 3 muestras por device/hora.
* **Detecci√≥n de outliers**: descartar mediciones fuera de 3œÉ antes de promediar.
* **TTL en Mongo**: colecci√≥n auxiliar con TTL para lecturas ‚Äúcrudas‚Äù y colecci√≥n agregada por hora para hist√≥rico.

---
